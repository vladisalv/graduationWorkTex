\chapter{Результаты вычислительных экспериментов} \label{chapt3}

Все последующие эксперименты, за редким исключением,
выполнялись на суперкомпьютере Ломоносов на базе Intel® Xeon 5570 и графических
процессоров Tesla X2070 (поколение Fermi).
Если запуск производился с использованием графических процессоров,
то на каждое устройство приходилось по одному mpi-процессу, если не оговорено
обратного.

В дальнейшем программа с использованием графических процессоров будет называться
GPU-программой, а без использования - HOST-программой. Программа с построением
матрицы гомологии и ее последующим анализом - матричной программой, а программу,
использующую блочный метод (одновременное построение блока матрицы и его анализ),
будем называть блочной программой.

В качестве входных данных использовалась
искусственно сгенерированная тестовая биологическая последовательность длинной
5 млн. символов, содержащая в себе 2 протяженных повтора. Данная биологическая
последовательность сравнивалась сама с собой.

В качестве параметров алгоритма использовались следующие значения:
\begin{itemize}
    \item Длина окна профилирования: 250
    \item Длина окна спектрального индексирования: 250
    \item Шаг окна спектрального индексирования: 100
    \item Количество коэффициентов разложения: 75
    \item Точность поиска $\varepsilon$: 0.01
\end{itemize}

\clearpage

\section{Сравнение блочной программы с матричной}

\begin{figure}[h]
    \center{\includegraphics[width=0.50\linewidth]{image/blockA.png}}
    \caption{Зависимость времени работы блочной программы от размера блока}\label{img:blockA}
\end{figure}

\bigskip

Как видно из графика \ref{img:blockA} для блочной HOST-программы не имеет значения
размер вычисляемого блока. Однако для GPU-программы размер блока имеет
принципиальное значение - именно размер блока определяет загруженность
устройства работой. Для эффективной работы GPU-программы нужен достаточно
большой размер блока - такой, чтобы устройство заработало в полную силу.
В тоже время не стоит увлекаться с увеличением размера блока,
т.к. на устройстве может попросту не хватить памяти.

Во всех последующих экспериментах мы будем использовать блоки~размером~10000х10000.

\begin{figure}[h]

    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/blockBH.png}}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/blockBG.png}}
    \end{minipage}

    \caption{Сравнение времени работы блочного и матричного метода}\label{img:blockB}
\end{figure}

Изначально предполагалось, что блочная программа будем дополнением к матричной,
чтобы можно было использовать программу на системах с небольшим объемом
оперативной памяти. Однако по результатам работы двух программ стало ясно,
что и для GPU-программы, и для HOST-программы во всех случаях
предпочтительнее использовать блочный вариант.

Стоит заранее оговорить, что использование блочного метода в GPU-программе дает лучшие результаты
по сравнению с матричным методом, если использовать конфигурацию при которой
на одно GPU устройство приходится больше одного mpi-процесса (см. рис. \ref{img:gpuC1} б.)

Также теоретически блочная программа будет лучше масштабироваться
на системах со слабой коммуникационной средой, т.к. имеет больше времени
на асинхронную передачу спектров (такое увеличение времени стало возможным
за счет объединения этапа построения гомологической матрицы и её анализа).

Во всех последующих экспериментах мы будем использовать блочную программу.

\section{Использование графических процессоров}

\begin{figure}[h]
\center{\includegraphics[width=0.6\linewidth]{image/gpuA.png}}
\caption{Сравнение HOST-программы и GPU-программы}\label{img:gpuA}
\end{figure}

На диаграмме \ref{img:gpuA} мы видим результат сравнения GPU-программы и HOST-программы,
запущенных на одном процессе.
Этап спектрального индексирования ускорился в 3 раза,
этап спектрального сравнения в 22 раза, программа в целом ускорилась в 5 раз.
Такие результаты ускорений значительно снизили долю этапов спектрального
индексирования и спектрального сравнения в программе.

\begin{figure}[h]
\center{\includegraphics[width=0.45\linewidth]{image/gpuB.png}}
\caption{Масштабируемость ускорения GPU-программы}\label{img:gpuB}
\end{figure}

На графике \ref{img:gpuB} мы видим изменение ускорения GPU-программы по сравнению с HOST-программой в зависимости
от числа процессов. Как и ожидалось - с ростом числа процессов ускорение падает.
Это объясняется уменьшением в вычислениях доли этапа спектрального сравнения,
а в последующем и недостаточной загруженностью графических процессоров.

\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/gpuC1a.png} \\ а}
    \label{img:gpuC1a}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/gpuC1b.png} \\ б}
    \label{img:gpuC1b}
    \end{minipage}
    \caption{Изменение времени выполнения этапов программы от количества процессов на устройство GPU}\label{img:gpuC1}
\end{figure}

Рассмотрим поведение GPU-программы, если на каждое GPU устройство будет приходиться больше одного mpi-процесса.
Так как каждый процесс занимает устройство на короткое время, а не блокирует его
на время выполнения всей программы, то эффективность использования GPU падает совсем незначительно.
Это даст хорошие результаты на практике, т.к. большинство кластеров имеют
в своем распоряжении большое число обычных процессоров и незначительное количество
графических плат. Даже на таких системах можно будет запускать программу
в конфигурации сильного разделения устройства (8 mpi x 1 GPU, 16 mpi x 1 GPU)
и все равно время выполнения GPU-программы будет меньше времени выполнения HOST-программы.
Результаты матричной программы хоть и не намного, но оказались хуже результатов
блочной программы.

\begin{figure}[h]
    \center{\includegraphics[width=0.6\linewidth]{image/gpuC2.png}}
    \caption{Масштабируемость алгоритма при использовании нескольких процессов на одно устройство}\label{img:gpuC2}
\end{figure}

Разделение устройства между несколькими процессами не оказывает влияния
на масштабируемость.








\section{Масштабируемость параллельного алгоритма}

Для следующих запусков использовалась искусственно сгенерированная
биологическая последовательность длинной 20 млн.

Кроме Ломоносова вычисления производились на суперкомпьютере Blue Gene/P (4xPowerPC 450).

\bigskip

\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masAa.png}}
    \caption{Сильная масштабируемость GPU- и HOST-программы}\label{img:masAa}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masAb.png}}
    \caption{Ускорение GPU- и HOST-программы}\label{img:masAa}
    \end{minipage}
\end{figure}

GPU-программа показала чуть меньшую масштабируемость. Это объясняется
уменьшением доли GPU-вычислений с уменьшением входных данных.


\bigskip


\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masBa.png}}
    \caption{Сильная масштабируемость HOST-программы}\label{img:masBa}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masBb.png}}
    \caption{Ускорение HOST-программы}\label{img:masBb}
    \end{minipage}
\end{figure}

Программа показала очень хорошую масштабируемость на системе BlueGene/P.
Во многом это произошло благодаря архитектуре системы, а так же отсутствию
в программе блокирующих операций.

\clearpage

На рисунках ниже представлены результаты работы системы
для анализа поведения параллельных программ, разрабатываемой Матвеевым Владимиром
в рамках спецсеминара “Суперкомпьютерная обработка данных
с использованием нейросетей и эволюционных вычислений”
под руководством Л.Н.Королева и Н.Н.Поповой.
Система позволяет визуализировать трассу параллельной программы,
на которой отображается появление коммуникационных событий исследуемой программы
во времени. Также система позволяет просматривать различные коммуникационные матрицы,
в данном случае, по суммарному размеру сообщений и суммарному времени передачи
сообщений между процессами.

\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/comMat.png}}
    \caption{\\
        Коммуникационная матрица взаимодействия по размеру передаваемых данных}\label{img:comMat}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/comMatTime.png}}
    \caption{\\
        Коммуникационная матрица взаимодействия по времени}\label{img:comMatTime}
    \end{minipage}
\end{figure}

Из рисунка \ref{img:comMat} видно, что все процессы обмениваются почти одинаковым количеством данных.

\begin{figure}[h]
    \center{\includegraphics[width=1\linewidth]{image/tras.png}}
    \caption{Трасса программы}\label{img:tras}
\end{figure}

Трасса из рисунка \ref{img:tras} очень хорошо отображает общую структуру программы:
\begin{itemize}
    \item Обмен по топологии кольца (два этапа профилирования можно разглядеть в самом начале)
    \item Асинхронный обмен ``каждый-с-каждым'' который выполняется при анализе
        два раза для GC и GA профилей.
\end{itemize}

\clearpage

\section{Верификация полученных данных}

%\begin{figure}[h]
%    \begin{minipage}[h]{0.49\linewidth}
%    \center{\includegraphics[width=1\linewidth]{image/artificialGC.png}}
%    \caption{\\
%        Сравнение }\label{img:comMat}
%    \end{minipage}
%    \hfill
%    \begin{minipage}[h]{0.49\linewidth}
%    \center{\includegraphics[width=1\linewidth]{image/artificialMy.png}}
%    \caption{\\
%        Коммуникационная матрица взаимодействия по времени}\label{img:comMatTime}
%    \end{minipage}
%    \caption{ Гомологическая матрица сравнения искусственной последовательности с самой собой}
%\end{figure}

Ниже на рис. \ref{fig:artificial} и \ref{fig:rusmat} изображены гомологические
матрицы, полученные при помощи последовательной реализации спектрально-аналитического
метода и параллельной программы. Эксперименты проводились на системе Ломоносов.
Использовалась матричная HOST программа.

\begin{figure}[h]
\subfigure[]{
    \includegraphics[width=0.4\linewidth]{image/artificialGC.png}
    \label{fig:exp1a}
}
\hspace{4ex}
\subfigure[]{
    \includegraphics[width=0.4\linewidth]{image/artificialMy.png}
    \label{fig:exp1b}
}
\caption{Гомологическая матрица сравнения искусственной последовательности с самой собой.
    Длина последовательности 50 тыс.;
    \subref{fig:exp1a} последовательная программа;
    \subref{fig:exp1b} параллельная программа;
} \label{fig:artificial}
\end{figure}


\begin{figure}[h]
\subfigure[]{
    \includegraphics[width=0.4\linewidth]{image/ratmus.png}
    \label{fig:exp2a}
}
\hspace{4ex}
\subfigure[]{
    \includegraphics[width=0.4\linewidth]{image/myratmus.png}
    \label{fig:exp2b}
}
\caption{Гомологическая матрица сравнения Mus musculus и Rattus norvegicus.
    Длина последовательностей 172.5 Мб и 143.6 Мб соответственно.
    Параметры алгоритма: $W1: 300000, d1: 64, W2:800000, d2: 160000, coef: 5, \varepsilon 4*10^{-5}$;
    \subref{fig:exp2a} последовательная программа;
    \subref{fig:exp2b} параллельная программа;
} \label{fig:rusmat}
\end{figure}


\clearpage
