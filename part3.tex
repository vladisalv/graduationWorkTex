\chapter{Результаты вычислительных экспериментов} \label{chapt3}

Все последующие эксперименты, за редким исключением,
выполнялись на суперкомпьютере Ломоносов на базе Intel® Xeon 5570 и графических
процессоров Tesla X2070 (поколение Fermi).
Если запуск производился с использованием графических процессров,
то на каждое устройство приходилось по одному mpi-процессу, если не оговорено
обратного.

В дальнейшем программа с использованием графических процессоров будет называться
GPU-программой, а без использования - HOST-программой. Программа с построением
матрицы гомологии и ее последующим анализом - матричной программой, а программу,
использующую блочный метод (одновременное построение блока матрицы и его анализ),
будем называть блочной программой.

В качестве входных данных использовалась
искусственно сгенерированная тестовая биологическая последовательность длинной
5 млн. символов, содержащая в себе 2 протяженных повтора. Данная биологическая
последовательность сравнивалась сама с собой.

В качестве параметров алгоритма использовались следующие значения:
\begin{itemize}
    \item Длина окна профилирования: 250
    \item Длина окна спектрального индексирования: 250
    \item Шаг окна спектрального индексирования: 100
    \item Количество коэффициентов разложения: 75
    \item Точность поиска $\varepsilon$: 0.01
\end{itemize}

\clearpage

\section{Сравнение блочной программы с матричной}

\begin{figure}[h]
    \center{\includegraphics[width=0.60\linewidth]{image/blockA.png}}
    \caption{Зависимость времени работы блочной программы от размера блока}\label{img:blockA}
\end{figure}

\bigskip

Как видно из графика \ref{img:blockA} для блочной HOST-программы не имеет значения
размер вычисляемого блока. Однако для GPU-программы размер блока имеет
принципиальное значение - именно размер блока определяет загруженность
устройства работой. Для эффективной работы GPU-программы нужен достаточно
большой размер блока - такой, чтобы устройство заработало в полную силу.
В тоже время не стоит увлекаться с увеличением размера блока,
т.к. на устройстве может попросту не хватить памяти.

Во всех последующих экспериментах мы будем использовать блоки~размером~10000х10000.

\bigskip
\bigskip
\bigskip

\begin{figure}[h]

    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/blockBH.png}}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/blockBG.png}}
    \end{minipage}

    \caption{Сравнение времени работы блочного и матричного метода}\label{img:blockB}
\end{figure}

Изначально предполагалось, что блочная программа будем дополнением к матричной,
чтобы можно было использовать программу на системах с небольшим объемом
оперативной памяти. Однако по результатам работы двух программ стало ясно,
что и для GPU-программы, и для HOST-программы во всех случаях
предпочтительнее использовать блочный вариант.

Стоит заранее оговорить, что использование блочного метода в GPU-программе дает лучшие результаты
по сравнению с матричным методом, если использовать конфигурацию при которой
на одно GPU устройство приходится больше одного mpi-процесса (см. рис. \ref{img:gpuC1b}б).

Также теоретически блочная программа будет лучше масштабироваться
на системах со слабой коммуникационной средой, т.к. имеет больше времени
на ассинхнонную передачу спектров (такое увеличение времени стало возможным
за счет объединения этапа построения гомологической матрицы и её анализа).

Во всех последующих экспериментах мы будем использовать блочную программу.




\section{Использование графических процессоров}

\begin{figure}[h]
\center{\includegraphics[width=0.6\linewidth]{image/gpuA.png}}
\caption{Сравнение HOST-программы и GPU-программы}\label{img:gpuA}
\end{figure}

На диаграмме \ref{img:gpuA} мы видим результат сравнения GPU-программы и HOST-программы,
запущенных на одном процессе.
Этап спектрального индексирования ускорился в 3 раза,
этап спектрального сравнения в 22 раза, программа в целом ускорилась в 5 раз.
Такие результаты ускорений значительно снизили долю этапов спектрального
индексирования и спектрального сравнения в программе.

\begin{figure}[h]
\center{\includegraphics[width=0.45\linewidth]{image/gpuB.png}}
\caption{Масштабируемость ускорения GPU-программы}\label{img:gpuB}
\end{figure}

На графике \ref{img:gpuB} мы видим изменение ускорения GPU-программы по сравнению с HOST-программой в зависимости
от числа процессов. Как и ожидалось - с ростом числа процессов ускорение падает.
Это объясняется уменьшением в вычислениях доли этапа спектрального сравнения,
а в последующем и недостаточной загруженностью графических процессоров.

\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/gpuC1a.png} \\ а}
    \label{img:gpuC1a}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/gpuC1b.png} \\ б}
    \label{img:gpuC1b}
    \end{minipage}
    \caption{Изменение времени выполнения этапов программы от количества процессов на устройство GPU}\label{img:gpuC1}
\end{figure}

\bigskip

\bigskip

\bigskip

Рассмотрим поведение GPU-программы, если на каждое GPU устройство будет приходиться больше одного mpi-процесса.
Так как каждый процесс занимает устройство на короткое время, а не блокирует его
на время выполнения всей программы, то эффективность использования GPU падает совсем незначительно.
Это даст хорошие результаты на практике, т.к. большинство кластеров имеют
в своем распоряжении большое число обычных процессоров и незначительное количество
графических плат. Даже на таких системах можно будет запускать программу
в конфигурации сильного разделения устройства (8 mpi x 1 GPU, 16 mpi x 1 GPU)
и все равно время выполнения GPU-программы будет меньше времени выполнения HOST-программы.
Результаты матричной программы хоть и не намного, но оказались хуже результатов
блочной программы.
\clearpage

\begin{figure}[h]
    \center{\includegraphics[width=0.6\linewidth]{image/gpuC2.png}}
    \caption{Масштабируемость алгоритма при использовании нескольких процессов на одно устройство}\label{img:gpuC2}
\end{figure}

Разделение устройства между несколькими процессами не оказывает влияния
на масштабируемость.








\section{Масштабируемость параллельного алгоритма}

Для следующих запусков использовалась искусственно сгенерированная
биологическая последовательность длинной 20 млн.

Кроме Ломоносова вычисления производились на суперкомпьютере Blue Gene/P (4xPowerPC 450).

\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masAa.png}}
    \caption{Сильная масштабируемость GPU- и HOST-программы}\label{img:masAa}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masAb.png}}
    \caption{Ускорение GPU- и HOST-программы}\label{img:masAa}
    \end{minipage}
\end{figure}

GPU-программа показала чуть меньшую масштабируемость. Это объясняется
уменьшением доли GPU-вычислений с уменьшением входных данных.


\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masBa.png}}
    \caption{Сильная масштабируемость HOST-программы}\label{img:masBa}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/masBb.png}}
    \caption{Ускорение HOST-программы}\label{img:masBb}
    \end{minipage}
\end{figure}

Программа показала очень хорошую масштабируемость на системе BlueGene/P.
Во многом это произошло благодаря архитектуре системы, а так же отсутствию
в программе блокирующих операций.

На рисунках ниже представлены результаты работы системы
для анализа поведения параллельных программ, разрабатываемой Матвеевым Владимиром
в рамках спецсеминара “Суперкомпьютерная обработка данных
с использованием нейросетей и эволюционных вычислений”
под руководством Л.Н.Королева и Н.Н.Поповой.
Система позволяет визуализировать трассу параллельной программы,
на которой отображается появление коммуникационных событий исследуемой программы
во времени. Также система позволяет просматривать различные коммуникационные матрицы,
в данном случае, по суммарному размеру сообщений и суммарному времени передачи
сообщений между процессами.

\begin{figure}[h!]
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/comMat.png}}
    \caption{\\
        Коммуникационная матрица взаимодействия по размеру передаваемых данных}\label{img:comMat}
    \end{minipage}
    \hfill
    \begin{minipage}[h]{0.49\linewidth}
    \center{\includegraphics[width=1\linewidth]{image/comMatTime.png}}
    \caption{\\
        Коммуникационная матрица взаимодействия по времени}\label{img:comMatTime}
    \end{minipage}
\end{figure}

Из рисунка \ref{img:comMat} видно, что все процессы обмениваются почти одинаковым количеством данных.
\clearpage

\begin{figure}[h]
    \center{\includegraphics[width=1\linewidth]{image/tras.png}}
    \caption{Трасса программы}\label{img:tras}
\end{figure}

Трасса из рисунка \ref{img:tras} очень хорошо отображает общую структуру программы:
\begin{itemize}
    \item Обмен по топологии кольца (два этапа профилирования можно разглядеть в самом начале)
    \item Асинхронный обмен "каждый-с-каждым", который выполняется при анализе
        два раза для GC и GA профилей.
\end{itemize}

%\section{Правильность полученнных результатов}
\clearpage
    %\begin{minipage}[h]{0.49\linewidth}
    %\end{minipage}
